{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_comments_df = pd.read_csv(os.path.join(\"static\",\"data\",\"raw_comments_239858.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>body_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>depressedbutimlit</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>Haven't been in one, I'm 19</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Anonstarr</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>Male sea horses are the ones who actually get ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[male, sea, horse, actually, pregnant, give, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>div_tiw</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>Learnt polar form calculus</td>\n",
       "      <td>1</td>\n",
       "      <td>[learn, polar, form, calculus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>RAMI_XXL</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>I would say the fried chicken, ham, steak and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[fry, chicken, ham, steak, chocolate, fountain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>JesusChristSuperDerp</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>Wonder Boy</td>\n",
       "      <td>1</td>\n",
       "      <td>[wonder, boy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239853</td>\n",
       "      <td>Bensemus</td>\n",
       "      <td>technology</td>\n",
       "      <td>It also doesn’t take hundreds of millions or b...</td>\n",
       "      <td>13</td>\n",
       "      <td>[doe, hundred, million, billion, dollar, brand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239854</td>\n",
       "      <td>besselfunctions</td>\n",
       "      <td>technology</td>\n",
       "      <td>Thank you for a serious and thoughtful response.</td>\n",
       "      <td>4</td>\n",
       "      <td>[thank, serious, thoughtful, response]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239855</td>\n",
       "      <td>the_monkey_knows</td>\n",
       "      <td>technology</td>\n",
       "      <td>It’ll be dead by a thousand cuts compared to t...</td>\n",
       "      <td>2</td>\n",
       "      <td>[dead, thousand, cut, compare, swift, effect, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239856</td>\n",
       "      <td>I_am_the_night</td>\n",
       "      <td>technology</td>\n",
       "      <td>Ah yes, the party of small government</td>\n",
       "      <td>2</td>\n",
       "      <td>[yes, party, small, government]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239857</td>\n",
       "      <td>johnnybagels</td>\n",
       "      <td>technology</td>\n",
       "      <td>Well if what you're saying is true then their ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[true, strike, fail, employee, independent, co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239858 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      author   subreddit  \\\n",
       "0          depressedbutimlit   AskReddit   \n",
       "1                  Anonstarr   AskReddit   \n",
       "2                    div_tiw   AskReddit   \n",
       "3                   RAMI_XXL   AskReddit   \n",
       "4       JesusChristSuperDerp   AskReddit   \n",
       "...                      ...         ...   \n",
       "239853              Bensemus  technology   \n",
       "239854       besselfunctions  technology   \n",
       "239855      the_monkey_knows  technology   \n",
       "239856        I_am_the_night  technology   \n",
       "239857          johnnybagels  technology   \n",
       "\n",
       "                                                     body  score  \\\n",
       "0                             Haven't been in one, I'm 19      1   \n",
       "1       Male sea horses are the ones who actually get ...      1   \n",
       "2                              Learnt polar form calculus      1   \n",
       "3       I would say the fried chicken, ham, steak and ...      1   \n",
       "4                                              Wonder Boy      1   \n",
       "...                                                   ...    ...   \n",
       "239853  It also doesn’t take hundreds of millions or b...     13   \n",
       "239854   Thank you for a serious and thoughtful response.      4   \n",
       "239855  It’ll be dead by a thousand cuts compared to t...      2   \n",
       "239856              Ah yes, the party of small government      2   \n",
       "239857  Well if what you're saying is true then their ...      1   \n",
       "\n",
       "                                              body_tokens  \n",
       "0                                                      []  \n",
       "1       [male, sea, horse, actually, pregnant, give, b...  \n",
       "2                          [learn, polar, form, calculus]  \n",
       "3       [fry, chicken, ham, steak, chocolate, fountain...  \n",
       "4                                           [wonder, boy]  \n",
       "...                                                   ...  \n",
       "239853  [doe, hundred, million, billion, dollar, brand...  \n",
       "239854             [thank, serious, thoughtful, response]  \n",
       "239855  [dead, thousand, cut, compare, swift, effect, ...  \n",
       "239856                    [yes, party, small, government]  \n",
       "239857  [true, strike, fail, employee, independent, co...  \n",
       "\n",
       "[239858 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_comments_df[\"body_tokens\"] = raw_comments_df[\"body\"].apply(utils.preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(239858, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(raw_comments_df[\"body_tokens\"][0])\n",
    "raw_comments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only interested in rows where the length of the list of clean tokens is > 0, \n",
    "# i.e. distinguishing sentences with at least one non-stopword\n",
    "cleaned_comments_df = raw_comments_df.loc[raw_comments_df[\"body_tokens\"].astype(str) != \"[]\", :]\n",
    "cleaned_comments_df = cleaned_comments_df.reset_index(drop=True)\n",
    "cleaned_comments_df[\"body\"] = cleaned_comments_df[\"body\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author          True\n",
       "subreddit      False\n",
       "body           False\n",
       "score          False\n",
       "body_tokens     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_comments_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235575, 5)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing empty comments. maybe these comments really were \"NaN\"\n",
    "cleaned_comments_df = cleaned_comments_df.dropna(subset = [\"body_tokens\"]).reset_index(drop=True)\n",
    "cleaned_comments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yourself', 'not', 'look', 'time', 'being', 'very', 'thing', 'no', 'ain', 'our', 'from', \"weren't\", 'theirs', 'mightn', \"mustn't\", 'say', 'while', 'she', 'how', \"hadn't\", 'having', 'my', 'during', 'reddit.', 'any', 'with', 'same', \"shan't\", 'hasn', 'itself', 'his', 'to', 'okay', 'on', '...', 'said', 'people', 'want', 'should', 'yours', 'y', 'against', 'down', ' could', 'him', 'her', 'but', 'its', \"that'll\", \"doesn't\", 'at', 'we', 't', 'com', 'yeah', 'few', 'ourselves', 'are', 'he', 'before', 'which', 'into', 'such', 'name', 'http', 'can', 'll', 'doesn', 'until', 'as', 'their', 'become', 'between', 'www', 'shan', \"you've\", 'in', 've', 'hers', 'wasn', 'what', 'or', 'make', 'did', 'subreddit', 'know', 'for', 'above', 'hadn', 'reddit', 'too', 'needn', 'www.', 'an', \"won't\", 'over', 'won', 'they', 'do', 'own', 'them', 'further', \"wasn't\", 'wouldn', \"didn't\", 'when', \"wouldn't\", \"don't\", 'the', 'mustn', 'think', 'then', 'subject', 'of', 'again', 'your', 'now', 'after', 'it', 'these', \"shouldn't\", 'through', 'well', 'aren', 'both', 'shouldn', 'ours', 'most', '.com', \"it's\", 're', 'also', 'whom', 'm', 'come', 'why', 'himself', 'only', 'all', 'here', 'doing', \"aren't\", \"couldn't\", 'had', 'there', 'and', 'is', 'below', 'couldn', \"needn't\", 'use', 'yourselves', 'has', 'just', 'by', \"you'd\", 'up', 'been', 'am', 'where', 'each', \"should've\", \"mightn't\", 'be', \"you'll\", 'right', 'this', 'one', 'like', 'those', 'you.s', 'have', 'haven', 'nor', 'themselves', 'myself', 'don', 'off', 'will', 'd', \"isn't\", 'i', 'about', \"you're\", 'because', 'isn', 'some', \"she's\", 'didn', \"hasn't\", 'was', 'weren', 'a', \"haven't\", 'me', 'take', 'other', 'were', 'would', 'under', 'you', 'that', 'does', 'so', 'ma', 'who', 'if', 'herself', 'more', 'get', 'out', 's', 'than', 'o', 'once'}\n"
     ]
    }
   ],
   "source": [
    "print(utils.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_comments_df.to_csv(os.path.join(\"data\",\"cleaned_comments_235575.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_df = pd.DataFrame({\"Total Number of Tokens\": subreddit_tokens.apply(lambda x: len(x)),\n",
    "                        \"Number of Unique Tokens\": subreddit_tokens.apply(lambda x: len(set(x)))})\n",
    "\n",
    "explore_df[\"Lexical Diversity\"] = explore_df['Number of Unique Tokens'] / explore_df['Total Number of Tokens']\n",
    "\n",
    "explore_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "def wordListToFreqList(wordlist, top_n=10):\n",
    "    \"\"\"Compile a list of all words and their frequency of occurence\"\"\"\n",
    "    \n",
    "    # count each term's number of occurrences\n",
    "    freqDict = Counter(wordlist)\n",
    "    \n",
    "    # sort the frequency dictionary by its values descending and return the items as a list of tuples\n",
    "    sortedFreqs = sorted(freqDict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    return sortedFreqs[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = subreddit_tokens.apply(lambda tokens: wordListToFreqList(tokens))\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs.index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using gridspec allows us to dynamically add subplots in grid\n",
    "N = len(freqs.keys())\n",
    "cols = 2\n",
    "rows = int(math.ceil(N / cols))\n",
    "gs = gridspec.GridSpec(rows, cols)\n",
    "\n",
    "# define the figure space for the plots\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(N*2)\n",
    "fig.set_figwidth(20)\n",
    "\n",
    "# iterate over number of categories to plot each one's top terms\n",
    "for i in range(N):\n",
    "    \n",
    "    # add a plot to the figure\n",
    "    ax = fig.add_subplot(gs[i])\n",
    "    ax.set_title(f\"Most Frequent Words for: {freqs.index[i]}\", fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # break the terms and term counts into two lists/tuples\n",
    "    x,y = zip(*freqs[i])\n",
    "    #plot the data\n",
    "    ax.bar(x,y)\n",
    "    # increase x-label font size\n",
    "    plt.xticks(fontsize=14)\n",
    "    # place numeric label on the bar\n",
    "    for j, v in enumerate(y):\n",
    "        ax.text(j, v/2, str(v), color='white', fontweight='bold', ha='center')\n",
    "    \n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_comments_df[\"body_tokens_spaced\"] = cleaned_comments_df[\"body_tokens\"].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_comments_df.to_csv(os.path.join(\"static\",\"data\",\"cleaned_comments.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_comments_df = pd.read_csv(os.path.join(\"static\",\"data\",\"cleaned_comments.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer()\n",
    "tf = tf_vectorizer.fit_transform(cleaned_comments_df[\"body_tokens_spaced\"])\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 249\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=4).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic: {topic_idx}\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "n_top_words = 10\n",
    "display_topics(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
